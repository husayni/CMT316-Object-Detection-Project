{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T23:25:30.622201Z",
     "iopub.status.busy": "2025-04-29T23:25:30.621814Z",
     "iopub.status.idle": "2025-04-29T23:25:33.934211Z",
     "shell.execute_reply": "2025-04-29T23:25:33.933275Z",
     "shell.execute_reply.started": "2025-04-29T23:25:30.622169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install -q clearml optuna torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T23:25:33.936267Z",
     "iopub.status.busy": "2025-04-29T23:25:33.935995Z",
     "iopub.status.idle": "2025-04-29T23:25:34.302926Z",
     "shell.execute_reply": "2025-04-29T23:25:34.302322Z",
     "shell.execute_reply.started": "2025-04-29T23:25:33.936238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "os.environ['CLEARML_API_ACCESS_KEY'] = user_secrets.get_secret(\"CLEARML_API_ACCESS_KEY\")\n",
    "os.environ['CLEARML_API_SECRET_KEY'] = user_secrets.get_secret(\"CLEARML_API_SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T23:25:34.304254Z",
     "iopub.status.busy": "2025-04-29T23:25:34.303981Z",
     "iopub.status.idle": "2025-04-29T23:25:34.309938Z",
     "shell.execute_reply": "2025-04-29T23:25:34.309259Z",
     "shell.execute_reply.started": "2025-04-29T23:25:34.304227Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLEARML_WEB_HOST=https://app.clear.ml/\n",
      "env: CLEARML_API_HOST=https://api.clear.ml\n",
      "env: CLEARML_FILES_HOST=https://files.clear.ml\n"
     ]
    }
   ],
   "source": [
    "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
    "%env CLEARML_API_HOST=https://api.clear.ml\n",
    "%env CLEARML_FILES_HOST=https://files.clear.ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T23:25:34.311759Z",
     "iopub.status.busy": "2025-04-29T23:25:34.311517Z",
     "iopub.status.idle": "2025-04-29T23:25:34.326381Z",
     "shell.execute_reply": "2025-04-29T23:25:34.325631Z",
     "shell.execute_reply.started": "2025-04-29T23:25:34.311742Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from clearml import Task\n",
    "from optuna.exceptions import TrialPruned\n",
    "from PIL import Image\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchvision.models.detection import RetinaNet_ResNet50_FPN_Weights, retinanet_resnet50_fpn\n",
    "from torchvision.transforms import ColorJitter, Compose, InterpolationMode, Normalize, RandomHorizontalFlip, ToTensor\n",
    "from torchvision.transforms import functional as F\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(42)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T23:25:34.327452Z",
     "iopub.status.busy": "2025-04-29T23:25:34.327143Z",
     "iopub.status.idle": "2025-04-29T23:25:38.066908Z",
     "shell.execute_reply": "2025-04-29T23:25:38.065551Z",
     "shell.execute_reply.started": "2025-04-29T23:25:34.327397Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jupyter Notebook auto-logging failed, could not access: /kaggle/working/__notebook_source__.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=da8c7f696aef4279887bddd977577201\n",
      "ClearML results page: https://app.clear.ml/projects/31ab205b5fdb489d9ad1b4ed44a65563/experiments/da8c7f696aef4279887bddd977577201/output/log\n"
     ]
    }
   ],
   "source": [
    "args = SimpleNamespace(\n",
    "    voc_root=\"/kaggle/input/pascal-voc-2012/VOC2012\",\n",
    "    output_dir=\"/kaggle/working/optuna_outputs\",\n",
    "    train_set=\"train\",\n",
    "    val_set='val',\n",
    "    epochs=6,\n",
    "    batch_size=4,\n",
    "    momentum=0.9,\n",
    "    min_short_size=400,\n",
    "    max_short_size=800,\n",
    "    max_long_size=1333,\n",
    "    val_short_size=600,\n",
    "    val_long_size=1000,\n",
    "    workers=2,\n",
    "    trainable_backbone_layers=3,\n",
    "    resume_checkpoint=None,\n",
    "    n_trials=10,\n",
    "    study_name=\"retina_optuna\"\n",
    ")\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "task = Task.init(\n",
    "    project_name=\"CMT318-Object-Detection\",\n",
    "    task_name=\"RetinaNet_Optuna_Tuning\",\n",
    "    output_uri=None,\n",
    "    tags=[\"retinanet\",\"voc2012\",\"kaggle\",\"author:hussain\",\"account:kiran\",\"hyperparameter-tuning\"]\n",
    ")\n",
    "task.connect(vars(args))\n",
    "logger = task.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T23:25:38.069608Z",
     "iopub.status.busy": "2025-04-29T23:25:38.069209Z",
     "iopub.status.idle": "2025-04-29T23:25:45.740229Z",
     "shell.execute_reply": "2025-04-29T23:25:45.739649Z",
     "shell.execute_reply.started": "2025-04-29T23:25:38.069562Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class IdentityTransform(nn.Module):\n",
    "    def forward(self, images, targets):\n",
    "        return images, targets\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    'aeroplane','bicycle','bird','boat','bottle',\n",
    "    'bus','car','cat','chair','cow',\n",
    "    'diningtable','dog','horse','motorbike','person',\n",
    "    'pottedplant','sheep','sofa','train','tvmonitor'\n",
    "]\n",
    "CLASS_NAME_TO_IDX = {name: i+1 for i,name in enumerate(CLASS_NAMES)}\n",
    "NUM_CLASSES = len(CLASS_NAMES) + 1\n",
    "\n",
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, root, image_set, transforms=None):\n",
    "        self.root = root\n",
    "        ids_file = os.path.join(root, 'ImageSets', 'Main', f\"{image_set}.txt\")\n",
    "        with open(ids_file) as f:\n",
    "            img_ids = [l.strip() for l in f if l.strip()]\n",
    "        self.annotations = []\n",
    "        for img_id in img_ids:\n",
    "            xml_path = os.path.join(root, 'Annotations', f\"{img_id}.xml\")\n",
    "            tree = ET.parse(xml_path)\n",
    "            boxes, labels = [], []\n",
    "            for obj in tree.getroot().findall('object'):\n",
    "                cls = obj.find('name').text\n",
    "                idx = CLASS_NAME_TO_IDX.get(cls)\n",
    "                if idx is None:\n",
    "                    continue\n",
    "                b = obj.find('bndbox')\n",
    "                coords = [float(b.find('xmin').text), float(b.find('ymin').text),\n",
    "                          float(b.find('xmax').text), float(b.find('ymax').text)]\n",
    "                if coords[2] <= coords[0] or coords[3] <= coords[1]:\n",
    "                    # skip degenerate\n",
    "                    continue\n",
    "                boxes.append(coords)\n",
    "                labels.append(idx)\n",
    "            if not labels:\n",
    "                continue\n",
    "            self.annotations.append({\n",
    "                'id': img_id,\n",
    "                'boxes': torch.tensor(boxes, dtype=torch.float32),\n",
    "                'labels': torch.tensor(labels, dtype=torch.int64)\n",
    "            })\n",
    "        assert self.annotations, f\"No annotations for {image_set}!\"\n",
    "        self.transforms = transforms\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    def __getitem__(self, idx):\n",
    "        ann = self.annotations[idx]\n",
    "        img = Image.open(os.path.join(self.root, 'JPEGImages', f\"{ann['id']}.jpg\")).convert('RGB')\n",
    "        target = {'boxes': ann['boxes'].clone(), 'labels': ann['labels'].clone()}\n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target\n",
    "\n",
    "mean=[0.485,0.456,0.406]; std=[0.229,0.224,0.225]\n",
    "train_color_aug = Compose([ColorJitter(0.3,0.3,0.3,0.1)])\n",
    "\n",
    "def get_train_transform(min_s, max_s_short, max_l_long, mean, std):\n",
    "    def transform(img, target):\n",
    "        w,h = img.size\n",
    "        short = random.randint(min_s, max_s_short)\n",
    "        if w<h:\n",
    "            new_w,new_h = short, int(short*h/w)\n",
    "        else:\n",
    "            new_h,new_w = short, int(short*w/h)\n",
    "        long_dim = max(new_w,new_h)\n",
    "        if long_dim>max_l_long:\n",
    "            scale = max_l_long/long_dim\n",
    "            new_w, new_h = int(new_w*scale), int(new_h*scale)\n",
    "        img = F.resize(img, [new_h,new_w], interpolation=InterpolationMode.BILINEAR)\n",
    "        sx,sy = new_w/w, new_h/h\n",
    "\n",
    "        boxes = target['boxes'].clone()\n",
    "        boxes[:,[0,2]] *= sx; boxes[:,[1,3]] *= sy\n",
    "\n",
    "        img = train_color_aug(img)\n",
    "        if random.random()<0.5:\n",
    "            img = F.hflip(img)\n",
    "            x1,x2 = boxes[:,0].clone(), boxes[:,2].clone()\n",
    "            boxes[:,0] = new_w - x2; boxes[:,2] = new_w - x1\n",
    "\n",
    "        boxes[:,[0,2]].clamp_(0,new_w)\n",
    "        boxes[:,[1,3]].clamp_(0,new_h)\n",
    "        keep = (boxes[:,2] - boxes[:,0] > 0) & (boxes[:,3] - boxes[:,1] > 0)\n",
    "        boxes = boxes[keep]\n",
    "        target['labels'] = target['labels'][keep]\n",
    "\n",
    "        target['boxes'] = boxes\n",
    "        img = ToTensor()(img)\n",
    "        img = Normalize(mean,std)(img)\n",
    "        return img, target\n",
    "    return transform\n",
    "\n",
    "\n",
    "def get_val_transform(val_s, max_l_long, mean, std):\n",
    "    def transform(img,target):\n",
    "        w,h = img.size\n",
    "        if w<h:\n",
    "            new_w,new_h = val_s, int(val_s*h/w)\n",
    "        else:\n",
    "            new_h,new_w = val_s, int(val_s*w/h)\n",
    "        long_dim = max(new_w,new_h)\n",
    "        if long_dim>max_l_long:\n",
    "            scale = max_l_long/long_dim\n",
    "            new_w,new_h = int(new_w*scale), int(new_h*scale)\n",
    "        img = F.resize(img,[new_h,new_w], interpolation=InterpolationMode.BILINEAR)\n",
    "        sx,sy = new_w/w, new_h/h\n",
    "        boxes = target['boxes'].clone()\n",
    "        boxes[:,[0,2]]*=sx; boxes[:,[1,3]]*=sy\n",
    "        boxes[:,[0,2]].clamp_(0,new_w); boxes[:,[1,3]].clamp_(0,new_h)\n",
    "        keep = (boxes[:,2] - boxes[:,0] > 0) & (boxes[:,3] - boxes[:,1] > 0)\n",
    "        boxes = boxes[keep]\n",
    "        target['labels'] = target['labels'][keep]\n",
    "        target['boxes'] = boxes\n",
    "        img = ToTensor()(img)\n",
    "        img = Normalize(mean,std)(img)\n",
    "        return img, target\n",
    "    return transform\n",
    "\n",
    "def collate_fn(batch): return tuple(zip(*batch))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_map(model, loader, device):\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision(box_format='xyxy')\n",
    "    for imgs, tars in loader:\n",
    "        imgs = [i.to(device) for i in imgs]\n",
    "        outs = model(imgs)\n",
    "        preds = []\n",
    "        for o in outs:\n",
    "            preds.append({k: v.cpu() for k,v in o.items() if k in ['boxes','scores','labels']})\n",
    "        metric.update(preds, tars)\n",
    "    return metric.compute()\n",
    "\n",
    "def train_one_epoch(model, optimizer, loader, device, scheduler, epoch, scaler):\n",
    "    model.train()\n",
    "    total, cnt = 0.0, 0\n",
    "    for imgs,tars in tqdm(loader, desc=f\"Train Ep{epoch}\", leave=False):\n",
    "        imgs = [i.to(device) for i in imgs]\n",
    "        tars = [{k:v.to(device) for k,v in t.items()} for t in tars]\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            losses = model(imgs, tars)\n",
    "            loss = sum(losses.values())\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        total += loss.item(); cnt += 1\n",
    "    avg = total/cnt\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    logger.report_scalar(\"Loss\",\"train\",iteration=epoch,value=avg)\n",
    "    logger.report_scalar(\"LearningRate\",\"train\",iteration=epoch,value=lr)\n",
    "    print(f\"Epoch {epoch} done, lr={lr:.6f}, avg_loss={avg:.4f}\")\n",
    "\n",
    "train_transform = get_train_transform(\n",
    "    args.min_short_size,\n",
    "    args.max_short_size,\n",
    "    args.max_long_size,\n",
    "    mean, std\n",
    ")\n",
    "\n",
    "val_transform = get_val_transform(\n",
    "    args.val_short_size,\n",
    "    args.val_long_size,\n",
    "    mean, std\n",
    ")\n",
    "\n",
    "train_ds = VOCDataset(args.voc_root, args.train_set, transforms=train_transform)\n",
    "val_ds = VOCDataset(args.voc_root, args.val_set, transforms=val_transform)\n",
    "\n",
    "weights=[]\n",
    "counts={i:0 for i in range(1,NUM_CLASSES)}\n",
    "for ann in train_ds.annotations:\n",
    "    for lbl in ann['labels'].tolist(): counts[lbl]+=1\n",
    "for k,v in counts.items(): counts[k] = v or 1\n",
    "for ann in train_ds.annotations:\n",
    "    w = sum(1.0/counts[lbl] for lbl in ann['labels'].tolist())/len(ann['labels'])\n",
    "    weights.append(w)\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    wd = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
    "    task.connect({'lr':lr,'weight_decay':wd})\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=args.batch_size,\n",
    "        sampler=WeightedRandomSampler(weights, len(weights), True),\n",
    "        num_workers=args.workers, pin_memory=torch.cuda.is_available(), collate_fn=collate_fn\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=args.batch_size, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=torch.cuda.is_available(), collate_fn=collate_fn\n",
    "    )\n",
    "    steps = len(train_loader)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = retinanet_resnet50_fpn(\n",
    "        weights=None,\n",
    "        weights_backbone=ResNet50_Weights.IMAGENET1K_V1,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        trainable_backbone_layers=args.trainable_backbone_layers\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = SGD(model.parameters(), lr=lr, momentum=args.momentum, weight_decay=wd)\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer, max_lr=lr, epochs=args.epochs, steps_per_epoch=steps, pct_start=0.3, div_factor=25\n",
    "    )\n",
    "    scaler = torch.amp.GradScaler('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    start_ep = 1\n",
    "    if args.resume_checkpoint and os.path.exists(args.resume_checkpoint):\n",
    "        ck = torch.load(args.resume_checkpoint)\n",
    "        model.load_state_dict(ck['model_state_dict'])\n",
    "        optimizer.load_state_dict(ck['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(ck['scheduler_state_dict'])\n",
    "        scaler.load_state_dict(ck.get('scaler_state_dict', {}))\n",
    "        start_ep = ck.get('epoch', 1) + 1\n",
    "        if hasattr(scheduler, 'total_steps'):\n",
    "            expected = args.epochs * steps\n",
    "            assert scheduler.total_steps == expected, \\\n",
    "                f\"Scheduler total_steps {scheduler.total_steps} != expected {expected}\"\n",
    "\n",
    "    best_map = 0.0\n",
    "    best_stats = None\n",
    "    for ep in range(start_ep, args.epochs+1):\n",
    "        train_one_epoch(model, optimizer, train_loader, device, scheduler, ep, scaler)\n",
    "        mAPs = evaluate_map(model, val_loader, device)\n",
    "        overall = mAPs['map'].item()\n",
    "        trial.report(overall, ep)\n",
    "        if trial.should_prune():\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "                'epoch': ep\n",
    "            }, os.path.join(args.output_dir, f'resume_trial{trial.number}_ep{ep}.pth'))\n",
    "            del model; torch.cuda.empty_cache()\n",
    "            raise TrialPruned()\n",
    "        for k,v in mAPs.items():\n",
    "            logger.report_scalar('DetectionMetrics', k, iteration=ep, value=v.item())\n",
    "        if overall > best_map:\n",
    "            best_map, best_stats = overall, mAPs\n",
    "            ckpt = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "                'epoch': ep\n",
    "            }\n",
    "            path = os.path.join(args.output_dir, f'trial{trial.number}_best_ep{ep}.pth')\n",
    "            torch.save(ckpt, path)\n",
    "\n",
    "    del model; torch.cuda.empty_cache()\n",
    "    logger.report_text(f\"Trial {trial.number} done: best_map={best_map:.4f}\")\n",
    "    logger.report_text(f\"Best stats: {best_stats}\")\n",
    "    return best_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T23:25:45.741748Z",
     "iopub.status.busy": "2025-04-29T23:25:45.741184Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 23:25:45,999] A new study created in RDB with name: retina_optuna\n",
      "Train Ep1:   4%|▍         | 54/1430 [00:37<16:03,  1.43it/s]"
     ]
    }
   ],
   "source": [
    "study_storage = f\"sqlite:///{os.path.join(args.output_dir,args.study_name)}.db\"\n",
    "study = optuna.create_study(direction='maximize', storage=study_storage, study_name=args.study_name, load_if_exists=True)\n",
    "study.optimize(objective, n_trials=args.n_trials)\n",
    "\n",
    "best = study.best_trial\n",
    "print(f\"Best lr={best.params['lr']:.4e}, wd={best.params['weight_decay']:.4e} -> mAP={best.value:.4f}\")\n",
    "logger.report_text(f\"Study best: {best.params}, mAP={best.value:.4f}\")\n",
    "\n",
    "pattern = f\"trial{best.number}_best_ep*.pth\"\n",
    "matches = glob.glob(os.path.join(args.output_dir,pattern))\n",
    "if matches:\n",
    "    ckpt = sorted(matches)[-1]\n",
    "    dst = os.path.join(args.output_dir,'global_best_model.pth')\n",
    "    torch.save(torch.load(ckpt), dst)\n",
    "    task.upload_artifact(name='global_best_model.pth', artifact_object=dst)\n",
    "    print(f\"Global best model saved to {dst}\")\n",
    "with open(os.path.join(args.output_dir,'best_params.txt'),'w') as f:\n",
    "    f.write(str(best.params))\n",
    "\n",
    "task.close()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 35388,
     "sourceId": 47853,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
