{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":47853,"sourceType":"datasetVersion","datasetId":35388}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q clearml torchmetrics pillow","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:03:47.089950Z","iopub.execute_input":"2025-05-01T01:03:47.090195Z","iopub.status.idle":"2025-05-01T01:05:06.908427Z","shell.execute_reply.started":"2025-05-01T01:03:47.090177Z","shell.execute_reply":"2025-05-01T01:05:06.907497Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsigstore 3.6.1 requires rich~=13.0, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nos.environ['CLEARML_API_ACCESS_KEY'] = user_secrets.get_secret(\"CLEARML_API_ACCESS_KEY\")\nos.environ['CLEARML_API_SECRET_KEY'] = user_secrets.get_secret(\"CLEARML_API_SECRET_KEY\")\n%env CLEARML_WEB_HOST=https://app.clear.ml/\n%env CLEARML_API_HOST=https://api.clear.ml\n%env CLEARML_FILES_HOST=https://files.clear.ml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:05:06.909502Z","iopub.execute_input":"2025-05-01T01:05:06.909802Z","iopub.status.idle":"2025-05-01T01:05:07.847817Z","shell.execute_reply.started":"2025-05-01T01:05:06.909769Z","shell.execute_reply":"2025-05-01T01:05:07.847084Z"}},"outputs":[{"name":"stdout","text":"env: CLEARML_WEB_HOST=https://app.clear.ml/\nenv: CLEARML_API_HOST=https://api.clear.ml\nenv: CLEARML_FILES_HOST=https://files.clear.ml\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport random\nimport time\nimport xml.etree.ElementTree as ET\nfrom types import SimpleNamespace\n\nimport torch\nfrom clearml import Task\nfrom PIL import Image, ImageDraw\nfrom torch import nn\nfrom torch.optim import SGD\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\nfrom torchvision.models import ResNet50_Weights\nfrom torchvision.models.detection import retinanet_resnet50_fpn\nfrom torchvision.transforms import ColorJitter, InterpolationMode, Normalize, ToTensor\nfrom torchvision.transforms import functional as F\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:05:07.849515Z","iopub.execute_input":"2025-05-01T01:05:07.849776Z","iopub.status.idle":"2025-05-01T01:05:19.560122Z","shell.execute_reply.started":"2025-05-01T01:05:07.849760Z","shell.execute_reply":"2025-05-01T01:05:19.559320Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"args = SimpleNamespace(\n    # Paths\n    voc_root=\"/kaggle/input/pascal-voc-2012/VOC2012\",\n    base_output_dir=\"/kaggle/working/runs\",\n    train_set=\"train\",\n    val_set=\"val\",\n    resume_checkpoint=None,\n\n    # Training hyperparameters\n    epochs=50,\n    batch_size=4,\n    lr=1e-3,\n    weight_decay=1e-4,\n    momentum=0.9,\n\n    # Image resize / augmentation\n    min_short_size=400,\n    max_short_size=800,\n    max_long_size=1333,\n    val_short_size=600,\n    val_long_size=1000,\n\n    # Data loader\n    workers=2,\n\n    # Model\n    trainable_backbone_layers=3,\n)\n\nos.makedirs(args.base_output_dir, exist_ok=True)\n\nexisting = [\n    d for d in os.listdir(args.base_output_dir)\n    if os.path.isdir(os.path.join(args.base_output_dir, d)) and d.isdigit()\n]\n\nnums = sorted(int(d) for d in existing)\nnext_num = nums[-1] + 1 if nums else 1\n\nnew_output = os.path.join(args.base_output_dir, str(next_num))\nos.makedirs(new_output, exist_ok=False)\nargs.output_dir = new_output\n\nprint(f\"Writing run outputs to: {args.output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:05:19.561010Z","iopub.execute_input":"2025-05-01T01:05:19.561515Z","iopub.status.idle":"2025-05-01T01:05:19.569452Z","shell.execute_reply.started":"2025-05-01T01:05:19.561474Z","shell.execute_reply":"2025-05-01T01:05:19.568686Z"}},"outputs":[{"name":"stdout","text":"Writing run outputs to: /kaggle/working/runs/1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"experiment_tags = [\n    \"model_name:retinanet\",\n    \"dataset:voc2012\",\n    \"platform:kaggle\",\n    \"author:hussain\",\n    \"account:hussainsyed.dev@gmail.com\",\n    \"training\"\n]\n\ntask = Task.init(\n    project_name=\"CMT318-Object-Detection\",\n    task_name=\"RetinaNet_Training\",\n    tags=experiment_tags,\n    reuse_last_task_id=False\n)\ntask.connect(vars(args))\nlogger = task.get_logger()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:05:19.570280Z","iopub.execute_input":"2025-05-01T01:05:19.570541Z","iopub.status.idle":"2025-05-01T01:05:42.345071Z","shell.execute_reply.started":"2025-05-01T01:05:19.570515Z","shell.execute_reply":"2025-05-01T01:05:42.343887Z"}},"outputs":[{"name":"stdout","text":"ClearML Task: created new task id=f1ba947541e74e9c8c376960e8ab45fe\n2025-05-01 01:05:24,114 - clearml.Repository Detection - WARNING - Jupyter Notebook auto-logging failed, could not access: /kaggle/working/__notebook_source__.ipynb\n2025-05-01 01:05:24,136 - clearml.Task - INFO - Storing jupyter notebook directly as code\n","output_type":"stream"},{"name":"stderr","text":"2025-05-01 01:05:27.657571: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746061527.852829      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746061527.914118      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ClearML results page: https://app.clear.ml/projects/31ab205b5fdb489d9ad1b4ed44a65563/experiments/f1ba947541e74e9c8c376960e8ab45fe/output/log\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:05:42.346939Z","iopub.execute_input":"2025-05-01T01:05:42.348706Z","iopub.status.idle":"2025-05-01T01:05:42.425284Z","shell.execute_reply.started":"2025-05-01T01:05:42.348673Z","shell.execute_reply":"2025-05-01T01:05:42.424616Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"CLASS_NAMES = [\n    'aeroplane','bicycle','bird','boat','bottle',\n    'bus','car','cat','chair','cow',\n    'diningtable','dog','horse','motorbike','person',\n    'pottedplant','sheep','sofa','train','tvmonitor'\n]\nCLASS_NAME_TO_IDX = {name: i+1 for i, name in enumerate(CLASS_NAMES)}\nNUM_CLASSES = len(CLASS_NAMES) + 1  # +1 for background","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:05:42.426204Z","iopub.execute_input":"2025-05-01T01:05:42.426494Z","iopub.status.idle":"2025-05-01T01:05:42.438816Z","shell.execute_reply.started":"2025-05-01T01:05:42.426467Z","shell.execute_reply":"2025-05-01T01:05:42.438237Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"mean = [0.485, 0.456, 0.406]\nstd  = [0.229, 0.224, 0.225]\ncolor_aug = ColorJitter(0.3, 0.3, 0.3, 0.1)\n\ndef get_train_transform(min_s, max_short, max_long, mean, std):\n    def transform(img, target):\n        w, h = img.size\n        short = random.randint(min_s, max_short)\n        if w < h:\n            new_w, new_h = short, int(short * h / w)\n        else:\n            new_h, new_w = short, int(short * w / h)\n        long_dim = max(new_w, new_h)\n        if long_dim > max_long:\n            scale = max_long / long_dim\n            new_w, new_h = int(new_w * scale), int(new_h * scale)\n\n        img = F.resize(img, [new_h, new_w], interpolation=InterpolationMode.BILINEAR)\n        sx, sy = new_w / w, new_h / h\n\n        boxes = target['boxes'].clone()\n        boxes[:, [0,2]] *= sx\n        boxes[:, [1,3]] *= sy\n\n        img = color_aug(img)\n        if random.random() < 0.5:\n            img = F.hflip(img)\n            x1, x2 = boxes[:,0].clone(), boxes[:,2].clone()\n            boxes[:,0] = new_w - x2\n            boxes[:,2] = new_w - x1\n\n        boxes[:, [0,2]].clamp_(0, new_w)\n        boxes[:, [1,3]].clamp_(0, new_h)\n        keep = (boxes[:,2] - boxes[:,0] > 0) & (boxes[:,3] - boxes[:,1] > 0)\n        boxes = boxes[keep]\n\n        target['boxes'] = boxes\n        target['labels'] = target['labels'][keep]\n\n        img = ToTensor()(img)\n        img = Normalize(mean, std)(img)\n        return img, target\n\n    return transform\n\ndef get_val_transform(short_size, max_long, mean, std):\n    def transform(img, target):\n        w, h = img.size\n        if w < h:\n            new_w, new_h = short_size, int(short_size * h / w)\n        else:\n            new_h, new_w = short_size, int(short_size * w / h)\n        long_dim = max(new_w, new_h)\n        if long_dim > max_long:\n            scale = max_long / long_dim\n            new_w, new_h = int(new_w * scale), int(new_h * scale)\n\n        img = F.resize(img, [new_h, new_w], interpolation=InterpolationMode.BILINEAR)\n        sx, sy = new_w / w, new_h / h\n\n        boxes = target['boxes'].clone()\n        boxes[:, [0,2]] *= sx\n        boxes[:, [1,3]] *= sy\n        boxes[:, [0,2]].clamp_(0, new_w)\n        boxes[:, [1,3]].clamp_(0, new_h)\n        keep = (boxes[:,2] - boxes[:,0] > 0) & (boxes[:,3] - boxes[:,1] > 0)\n        boxes = boxes[keep]\n\n        target['boxes'] = boxes\n        target['labels'] = target['labels'][keep]\n\n        img = ToTensor()(img)\n        img = Normalize(mean, std)(img)\n        return img, target\n\n    return transform","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:05:42.439707Z","iopub.execute_input":"2025-05-01T01:05:42.439926Z","iopub.status.idle":"2025-05-01T01:05:42.457454Z","shell.execute_reply.started":"2025-05-01T01:05:42.439905Z","shell.execute_reply":"2025-05-01T01:05:42.456805Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class VOCDataset(Dataset):\n    def __init__(self, root, image_set, transforms=None):\n        self.root = root\n        ids_file = os.path.join(root, \"ImageSets\", \"Main\", f\"{image_set}.txt\")\n        with open(ids_file) as f:\n            img_ids = [l.strip() for l in f if l.strip()]\n\n        self.annotations = []\n        skipped_images = 0\n        for img_id in img_ids:\n            xml_path = os.path.join(root, \"Annotations\", f\"{img_id}.xml\")\n            tree = ET.parse(xml_path)\n            boxes, labels = [], []\n            for obj in tree.getroot().findall(\"object\"):\n                cls = obj.find(\"name\").text\n                idx = CLASS_NAME_TO_IDX.get(cls)\n                if idx is None:\n                    continue\n                b = obj.find(\"bndbox\")\n                coords = [\n                    float(b.find(\"xmin\").text),\n                    float(b.find(\"ymin\").text),\n                    float(b.find(\"xmax\").text),\n                    float(b.find(\"ymax\").text),\n                ]\n                if coords[2] <= coords[0] or coords[3] <= coords[1]:\n                    continue\n                boxes.append(coords)\n                labels.append(idx)\n\n            if not labels:\n                print(f\"No labels found for image id: {img_id}\")\n                skipped_images = skipped_images+1\n                continue\n\n            self.annotations.append({\n                \"id\": img_id,\n                \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n                \"labels\": torch.tensor(labels, dtype=torch.int64),\n            })\n            \n        print(f\"{image_set}.txt: Skipped {skipped_images} images\")\n        assert self.annotations, f\"No annotations for split {image_set}\"\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        ann = self.annotations[idx]\n        img_path = os.path.join(self.root, \"JPEGImages\", f\"{ann['id']}.jpg\")\n        img = Image.open(img_path).convert(\"RGB\")\n        target = {\n            \"boxes\": ann[\"boxes\"].clone(),\n            \"labels\": ann[\"labels\"].clone()\n        }\n        if self.transforms:\n            img, target = self.transforms(img, target)\n        return img, target\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:05:42.460390Z","iopub.execute_input":"2025-05-01T01:05:42.460572Z","iopub.status.idle":"2025-05-01T01:05:42.477743Z","shell.execute_reply.started":"2025-05-01T01:05:42.460558Z","shell.execute_reply":"2025-05-01T01:05:42.477130Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_ds = VOCDataset(\n    args.voc_root,\n    args.train_set,\n    transforms=get_train_transform(\n        args.min_short_size, args.max_short_size,\n        args.max_long_size, mean, std\n    )\n)\nval_ds = VOCDataset(\n    args.voc_root,\n    args.val_set,\n    transforms=get_val_transform(\n        args.val_short_size, args.val_long_size, mean, std\n    )\n)\nprint(f\"Train examples: {len(train_ds)}, Val examples: {len(val_ds)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:05:42.478479Z","iopub.execute_input":"2025-05-01T01:05:42.478706Z","iopub.status.idle":"2025-05-01T01:06:41.756128Z","shell.execute_reply.started":"2025-05-01T01:05:42.478691Z","shell.execute_reply":"2025-05-01T01:06:41.755479Z"}},"outputs":[{"name":"stdout","text":"train.txt: Skipped 0 images\nval.txt: Skipped 0 images\nTrain examples: 5717, Val examples: 5823\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"counts = dict.fromkeys(range(1, NUM_CLASSES), 0)\nfor ann in train_ds.annotations:\n    for lbl in ann[\"labels\"].tolist():\n        counts[lbl] += 1\nfor k, v in counts.items():\n    counts[k] = max(1, v)\nweights = [\n    sum(1.0 / counts[lbl] for lbl in ann[\"labels\"].tolist()) / len(ann[\"labels\"])\n    for ann in train_ds.annotations\n]\nsampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:06:41.756967Z","iopub.execute_input":"2025-05-01T01:06:41.757302Z","iopub.status.idle":"2025-05-01T01:06:41.788110Z","shell.execute_reply.started":"2025-05-01T01:06:41.757278Z","shell.execute_reply":"2025-05-01T01:06:41.787514Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"train_loader = DataLoader(\n    train_ds,\n    batch_size=args.batch_size,\n    sampler=sampler,\n    num_workers=args.workers,\n    pin_memory=torch.cuda.is_available(),\n    collate_fn=collate_fn\n)\nval_loader = DataLoader(\n    val_ds,\n    batch_size=args.batch_size,\n    shuffle=False,\n    num_workers=args.workers,\n    pin_memory=torch.cuda.is_available(),\n    collate_fn=collate_fn\n)\nprint(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:06:41.788813Z","iopub.execute_input":"2025-05-01T01:06:41.789544Z","iopub.status.idle":"2025-05-01T01:06:41.796140Z","shell.execute_reply.started":"2025-05-01T01:06:41.789525Z","shell.execute_reply":"2025-05-01T01:06:41.795338Z"}},"outputs":[{"name":"stdout","text":"Train batches: 1430, Val batches: 1456\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"model = retinanet_resnet50_fpn(\n    weights=None,\n    weights_backbone=ResNet50_Weights.IMAGENET1K_V1,\n    num_classes=NUM_CLASSES,\n    trainable_backbone_layers=args.trainable_backbone_layers\n).to(device)\n\noptimizer = SGD(\n    model.parameters(),\n    lr=args.lr,\n    momentum=args.momentum,\n    weight_decay=args.weight_decay\n)\nscheduler = OneCycleLR(\n    optimizer,\n    max_lr=args.lr,\n    epochs=args.epochs,\n    steps_per_epoch=len(train_loader),\n    pct_start=0.3,\n    div_factor=25\n)\nscaler = torch.amp.GradScaler(device)\n\nprint(f\"Starting LR: {scheduler.get_last_lr()[0]:.2e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:06:41.796955Z","iopub.execute_input":"2025-05-01T01:06:41.797236Z","iopub.status.idle":"2025-05-01T01:06:48.189911Z","shell.execute_reply.started":"2025-05-01T01:06:41.797213Z","shell.execute_reply":"2025-05-01T01:06:48.189155Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 202MB/s]\n","output_type":"stream"},{"name":"stdout","text":"2025-05-01 01:06:43,776 - clearml.model - INFO - Selected model id: 77e21dda103a441a84d5a2593a92b5f1\nStarting LR: 4.00e-05\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"start_epoch = 1\nif args.resume_checkpoint and os.path.exists(args.resume_checkpoint):\n    ck = torch.load(args.resume_checkpoint, map_location=device)\n    model.load_state_dict(ck[\"model_state_dict\"])\n    optimizer.load_state_dict(ck[\"optimizer_state_dict\"])\n    scheduler.load_state_dict(ck[\"scheduler_state_dict\"])\n    scaler.load_state_dict(ck.get(\"scaler_state_dict\", {}))\n    start_epoch = ck.get(\"epoch\", 1) + 1\n    print(f\"Resumed from checkpoint '{args.resume_checkpoint}', starting at epoch {start_epoch}\")\n\n\ndef train_one_epoch(model, loader, optimizer, scheduler, epoch, scaler, device):\n    model.train()\n    total_loss = 0.0\n    total_cls_loss = 0.0\n    total_box_loss = 0.0\n\n    for imgs, targets in tqdm(loader, desc=f\"Train Epoch {epoch}\", leave=False):\n        imgs = [img.to(device) for img in imgs]\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        optimizer.zero_grad()\n        with torch.amp.autocast(device):\n            loss_dict = model(imgs, targets)\n            # split out the two losses\n            cls_loss = loss_dict['classification']\n            box_loss = loss_dict['bbox_regression']\n            loss = cls_loss + box_loss\n\n        scaler.scale(loss).backward()\n        # (optional) clip gradients here if you wish\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n\n        total_loss     += loss.item()\n        total_cls_loss += cls_loss.item()\n        total_box_loss += box_loss.item()\n\n    n = len(loader)\n    avg_loss     = total_loss     / n\n    avg_cls_loss = total_cls_loss / n\n    avg_box_loss = total_box_loss / n\n    lr = scheduler.get_last_lr()[0]\n\n    print(f\"Epoch {epoch} | Avg Train Loss: {avg_loss:.4f} | Avg Cls Loss: {avg_cls_loss:.4f} | Avg Box Loss: {avg_box_loss:.4f} | LR: {lr:.2e}\")\n\n    logger.report_scalar(\"Train/Loss/Total\",          \"epoch\", iteration=epoch, value=avg_loss)\n    logger.report_scalar(\"Train/Loss/Classification\", \"epoch\", iteration=epoch, value=avg_cls_loss)\n    logger.report_scalar(\"Train/Loss/BoxRegression\",  \"epoch\", iteration=epoch, value=avg_box_loss)\n    logger.report_scalar(\"Train/LearningRate\",        \"epoch\", iteration=epoch, value=lr)\n\n\n@torch.no_grad()\ndef evaluate(model, loader, epoch):\n    model.eval()\n    metric = MeanAveragePrecision(box_format='xyxy', class_metrics=True)\n\n    total_val_loss     = 0.0\n    total_val_cls_loss = 0.0\n    total_val_box_loss = 0.0\n\n    for imgs, targets in tqdm(loader, desc=f\"Val Epoch {epoch}\", leave=False):\n        imgs_cuda    = [img.to(device) for img in imgs]\n        targets_cuda = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        model.train()\n        for m in model.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\n        loss_dict = model(imgs_cuda, targets_cuda)\n        cls_loss = loss_dict['classification']\n        box_loss = loss_dict['bbox_regression']\n        batch_loss = (cls_loss + box_loss).item()\n\n        total_val_loss     += batch_loss\n        total_val_cls_loss += cls_loss.item()\n        total_val_box_loss += box_loss.item()\n\n        model.eval()\n        outputs = model(imgs_cuda)\n        preds = [\n            {k: v.cpu() for k, v in out.items() if k in ('boxes','scores','labels')}\n            for out in outputs\n        ]\n        metric.update(preds, targets)\n\n    n = len(loader)\n    avg_val_loss     = total_val_loss     / n\n    avg_val_cls_loss = total_val_cls_loss / n\n    avg_val_box_loss = total_val_box_loss / n\n\n    results      = metric.compute()\n    overall_map  = results[\"map\"].item()\n    per_class_map = results[\"map_per_class\"]\n\n    print(f\"Epoch {epoch} | Avg Val Loss: {avg_val_loss:.4f} | Avg Cls Loss: {avg_val_cls_loss:.4f} | Avg Box Loss: {avg_val_box_loss:.4f} | mAP: {overall_map:.4f}\")\n\n    logger.report_scalar(\"Val/Loss/Total\",          \"epoch\", iteration=epoch, value=avg_val_loss)\n    logger.report_scalar(\"Val/Loss/Classification\", \"epoch\", iteration=epoch, value=avg_val_cls_loss)\n    logger.report_scalar(\"Val/Loss/BoxRegression\",  \"epoch\", iteration=epoch, value=avg_val_box_loss)\n    logger.report_scalar(\"Val/Detection/mAP\",       \"epoch\", iteration=epoch, value=overall_map)\n\n    for idx, ap in enumerate(per_class_map):\n        cls = CLASS_NAMES[idx]\n        logger.report_scalar(\"Val/Detection/AP\", cls, iteration=epoch, value=ap.item())\n\n    return avg_val_loss, overall_map\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:06:48.190725Z","iopub.execute_input":"2025-05-01T01:06:48.190939Z","iopub.status.idle":"2025-05-01T01:06:48.205824Z","shell.execute_reply.started":"2025-05-01T01:06:48.190919Z","shell.execute_reply":"2025-05-01T01:06:48.205022Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def log_head_histograms(epoch, model):\n    for name, param in model.named_parameters():\n        if (\"head.classification_head\" in name) or (\"head.regression_head\" in name):\n            if param.grad is not None:\n                logger.report_histogram(\n                    title=\"Gradients\",\n                    series=name,\n                    iteration=epoch,\n                    values=param.grad.detach().cpu().numpy().ravel(),\n                    data_args={\"nbinsx\": 50},        # Plotly’s nbinsx parameter\n                )\n            logger.report_histogram(\n                title=\"Weights\",\n                series=name,\n                iteration=epoch,\n                values=param.data.detach().cpu().numpy().ravel(),\n                data_args={\"nbinsx\": 50},\n            )\n\n\ndef log_sanity_images(epoch, model, val_loader, args, mean, std):\n    model.eval()\n    imgs, targets = next(iter(val_loader))\n    imgs_cuda = [img.to(device) for img in imgs]\n    with torch.no_grad():\n        outputs = model(imgs_cuda)\n\n    save_dir = os.path.join(args.output_dir, \"sanity_images\")\n    os.makedirs(save_dir, exist_ok=True)\n\n    for i, (img_tensor, tgt, pred) in enumerate(zip(imgs, targets, outputs)):\n        img = img_tensor.clone()\n        for c, m, s in zip(img, mean, std):\n            c.mul_(s).add_(m)\n        arr = (img.permute(1,2,0).cpu().numpy() * 255).astype(\"uint8\")\n        pil_img = Image.fromarray(arr)\n\n        draw = ImageDraw.Draw(pil_img)\n        for box in tgt[\"boxes\"]:\n            x1, y1, x2, y2 = box.tolist()\n            draw.rectangle([x1, y1, x2, y2], outline=\"green\", width=2)\n        for box in pred[\"boxes\"].cpu():\n            x1, y1, x2, y2 = box.tolist()\n            draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n\n        path = os.path.join(save_dir, f\"epoch{epoch}_sample{i}.png\")\n        pil_img.save(path)\n        logger.report_image(\n            title=\"Val Sample Detections\",\n            series=f\"epoch_{epoch}\",\n            local_path=path,\n            iteration=epoch\n        )\n\n    model.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:06:48.206706Z","iopub.execute_input":"2025-05-01T01:06:48.207162Z","iopub.status.idle":"2025-05-01T01:06:48.355095Z","shell.execute_reply.started":"2025-05-01T01:06:48.207137Z","shell.execute_reply":"2025-05-01T01:06:48.354279Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"best_map = 0.0\nfor epoch in range(start_epoch, args.epochs + 1):\n    epoch_start = time.time()\n    train_loss = train_one_epoch(\n        model, train_loader, optimizer, scheduler, epoch, scaler, device\n    )\n    \n    val_loss, val_map = evaluate(model, val_loader, epoch)\n    \n    if epoch % 2 == 0:\n        log_head_histograms(epoch, model)\n\n    if epoch % 5 == 0:\n        log_sanity_images(epoch, model, val_loader, args, mean, std)\n    \n    epoch_time = time.time() - epoch_start\n    logger.report_scalar(\n        title=\"Time/Epoch\",\n        series=\"duration_seconds\",\n        iteration=epoch,\n        value=epoch_time\n    )\n    if val_map > best_map:\n        best_map = val_map\n        best_ckpt = os.path.join(args.output_dir, \"best_model.pth\")\n        torch.save({\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n            \"scheduler_state_dict\": scheduler.state_dict(),\n            \"scaler_state_dict\": scaler.state_dict(),\n            \"epoch\": epoch\n        }, best_ckpt)\n        print(f\"Epoch {epoch}: new best mAP = {best_map:.4f}, saved to {best_ckpt}\")\n        task.upload_artifact(name=\"best_model.pth\", artifact_object=best_ckpt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:06:48.355897Z","iopub.execute_input":"2025-05-01T01:06:48.356126Z"}},"outputs":[{"name":"stderr","text":"Train Epoch 1:   8%|▊         | 120/1430 [01:25<14:21,  1.52it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"final_path = os.path.join(args.output_dir, \"final_model.pth\")\ntorch.save({\n    \"model_state_dict\": model.state_dict(),\n    \"optimizer_state_dict\": optimizer.state_dict(),\n    \"scheduler_state_dict\": scheduler.state_dict(),\n    \"scaler_state_dict\": scaler.state_dict(),\n    \"epoch\": args.epochs\n}, final_path)\nprint(f\"Final model weights saved to {final_path}\")\n\nbest_map_path = os.path.join(args.output_dir, \"best_map.txt\")\nwith open(best_map_path, \"w\") as f:\n    f.write(f\"best_map: {best_map:.4f}\\n\")\n\ntask.upload_artifact(name=\"final_model.pth\", artifact_object=final_path)\ntask.upload_artifact(name=\"best_map.txt\", artifact_object=best_map_path)\n\ntask.close()\nprint(\"Training complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}