{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-01T01:03:47.090195Z",
     "iopub.status.busy": "2025-05-01T01:03:47.089950Z",
     "iopub.status.idle": "2025-05-01T01:05:06.908427Z",
     "shell.execute_reply": "2025-05-01T01:05:06.907497Z",
     "shell.execute_reply.started": "2025-05-01T01:03:47.090177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q clearml torchmetrics pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:05:06.909802Z",
     "iopub.status.busy": "2025-05-01T01:05:06.909502Z",
     "iopub.status.idle": "2025-05-01T01:05:07.847817Z",
     "shell.execute_reply": "2025-05-01T01:05:07.847084Z",
     "shell.execute_reply.started": "2025-05-01T01:05:06.909769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "os.environ['CLEARML_API_ACCESS_KEY'] = user_secrets.get_secret(\"CLEARML_API_ACCESS_KEY\")\n",
    "os.environ['CLEARML_API_SECRET_KEY'] = user_secrets.get_secret(\"CLEARML_API_SECRET_KEY\")\n",
    "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
    "%env CLEARML_API_HOST=https://api.clear.ml\n",
    "%env CLEARML_FILES_HOST=https://files.clear.ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:05:07.849776Z",
     "iopub.status.busy": "2025-05-01T01:05:07.849515Z",
     "iopub.status.idle": "2025-05-01T01:05:19.560122Z",
     "shell.execute_reply": "2025-05-01T01:05:19.559320Z",
     "shell.execute_reply.started": "2025-05-01T01:05:07.849760Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "from clearml import Task\n",
    "from PIL import Image, ImageDraw\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchvision.models.detection import retinanet_resnet50_fpn\n",
    "from torchvision.transforms import ColorJitter, InterpolationMode, Normalize, ToTensor\n",
    "from torchvision.transforms import functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:05:19.561515Z",
     "iopub.status.busy": "2025-05-01T01:05:19.561010Z",
     "iopub.status.idle": "2025-05-01T01:05:19.569452Z",
     "shell.execute_reply": "2025-05-01T01:05:19.568686Z",
     "shell.execute_reply.started": "2025-05-01T01:05:19.561474Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    # Paths\n",
    "    voc_root=\"/kaggle/input/pascal-voc-2012/VOC2012\",\n",
    "    base_output_dir=\"/kaggle/working/runs\",\n",
    "    train_set=\"train\",\n",
    "    val_set=\"val\",\n",
    "    resume_checkpoint=None,\n",
    "\n",
    "    # Training hyperparameters\n",
    "    epochs=50,\n",
    "    batch_size=4,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    momentum=0.9,\n",
    "\n",
    "    # Image resize / augmentation\n",
    "    min_short_size=400,\n",
    "    max_short_size=800,\n",
    "    max_long_size=1333,\n",
    "    val_short_size=600,\n",
    "    val_long_size=1000,\n",
    "\n",
    "    # Data loader\n",
    "    workers=2,\n",
    "\n",
    "    # Model\n",
    "    trainable_backbone_layers=3,\n",
    ")\n",
    "\n",
    "os.makedirs(args.base_output_dir, exist_ok=True)\n",
    "\n",
    "existing = [\n",
    "    d for d in os.listdir(args.base_output_dir)\n",
    "    if os.path.isdir(os.path.join(args.base_output_dir, d)) and d.isdigit()\n",
    "]\n",
    "\n",
    "nums = sorted(int(d) for d in existing)\n",
    "next_num = nums[-1] + 1 if nums else 1\n",
    "\n",
    "new_output = os.path.join(args.base_output_dir, str(next_num))\n",
    "os.makedirs(new_output, exist_ok=False)\n",
    "args.output_dir = new_output\n",
    "\n",
    "print(f\"Writing run outputs to: {args.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:05:19.570541Z",
     "iopub.status.busy": "2025-05-01T01:05:19.570280Z",
     "iopub.status.idle": "2025-05-01T01:05:42.345071Z",
     "shell.execute_reply": "2025-05-01T01:05:42.343887Z",
     "shell.execute_reply.started": "2025-05-01T01:05:19.570515Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "experiment_tags = [\n",
    "    \"model_name:retinanet\",\n",
    "    \"dataset:voc2012\",\n",
    "    \"platform:kaggle\",\n",
    "    \"author:hussain\",\n",
    "    \"account:hussainsyed.dev@gmail.com\",\n",
    "    \"training\"\n",
    "]\n",
    "\n",
    "task = Task.init(\n",
    "    project_name=\"CMT318-Object-Detection\",\n",
    "    task_name=\"RetinaNet_Training\",\n",
    "    tags=experiment_tags,\n",
    "    reuse_last_task_id=False\n",
    ")\n",
    "task.connect(vars(args))\n",
    "logger = task.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:05:42.348706Z",
     "iopub.status.busy": "2025-05-01T01:05:42.346939Z",
     "iopub.status.idle": "2025-05-01T01:05:42.425284Z",
     "shell.execute_reply": "2025-05-01T01:05:42.424616Z",
     "shell.execute_reply.started": "2025-05-01T01:05:42.348673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:05:42.426494Z",
     "iopub.status.busy": "2025-05-01T01:05:42.426204Z",
     "iopub.status.idle": "2025-05-01T01:05:42.438816Z",
     "shell.execute_reply": "2025-05-01T01:05:42.438237Z",
     "shell.execute_reply.started": "2025-05-01T01:05:42.426467Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\n",
    "    'aeroplane','bicycle','bird','boat','bottle',\n",
    "    'bus','car','cat','chair','cow',\n",
    "    'diningtable','dog','horse','motorbike','person',\n",
    "    'pottedplant','sheep','sofa','train','tvmonitor'\n",
    "]\n",
    "CLASS_NAME_TO_IDX = {name: i+1 for i, name in enumerate(CLASS_NAMES)}\n",
    "NUM_CLASSES = len(CLASS_NAMES) + 1  # +1 for background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:05:42.439926Z",
     "iopub.status.busy": "2025-05-01T01:05:42.439707Z",
     "iopub.status.idle": "2025-05-01T01:05:42.457454Z",
     "shell.execute_reply": "2025-05-01T01:05:42.456805Z",
     "shell.execute_reply.started": "2025-05-01T01:05:42.439905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "color_aug = ColorJitter(0.3, 0.3, 0.3, 0.1)\n",
    "\n",
    "def get_train_transform(min_s, max_short, max_long, mean, std):\n",
    "    def transform(img, target):\n",
    "        w, h = img.size\n",
    "        short = random.randint(min_s, max_short)\n",
    "        if w < h:\n",
    "            new_w, new_h = short, int(short * h / w)\n",
    "        else:\n",
    "            new_h, new_w = short, int(short * w / h)\n",
    "        long_dim = max(new_w, new_h)\n",
    "        if long_dim > max_long:\n",
    "            scale = max_long / long_dim\n",
    "            new_w, new_h = int(new_w * scale), int(new_h * scale)\n",
    "\n",
    "        img = F.resize(img, [new_h, new_w], interpolation=InterpolationMode.BILINEAR)\n",
    "        sx, sy = new_w / w, new_h / h\n",
    "\n",
    "        boxes = target['boxes'].clone()\n",
    "        boxes[:, [0,2]] *= sx\n",
    "        boxes[:, [1,3]] *= sy\n",
    "\n",
    "        img = color_aug(img)\n",
    "        if random.random() < 0.5:\n",
    "            img = F.hflip(img)\n",
    "            x1, x2 = boxes[:,0].clone(), boxes[:,2].clone()\n",
    "            boxes[:,0] = new_w - x2\n",
    "            boxes[:,2] = new_w - x1\n",
    "\n",
    "        boxes[:, [0,2]].clamp_(0, new_w)\n",
    "        boxes[:, [1,3]].clamp_(0, new_h)\n",
    "        keep = (boxes[:,2] - boxes[:,0] > 0) & (boxes[:,3] - boxes[:,1] > 0)\n",
    "        boxes = boxes[keep]\n",
    "\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = target['labels'][keep]\n",
    "\n",
    "        img = ToTensor()(img)\n",
    "        img = Normalize(mean, std)(img)\n",
    "        return img, target\n",
    "\n",
    "    return transform\n",
    "\n",
    "def get_val_transform(short_size, max_long, mean, std):\n",
    "    def transform(img, target):\n",
    "        w, h = img.size\n",
    "        if w < h:\n",
    "            new_w, new_h = short_size, int(short_size * h / w)\n",
    "        else:\n",
    "            new_h, new_w = short_size, int(short_size * w / h)\n",
    "        long_dim = max(new_w, new_h)\n",
    "        if long_dim > max_long:\n",
    "            scale = max_long / long_dim\n",
    "            new_w, new_h = int(new_w * scale), int(new_h * scale)\n",
    "\n",
    "        img = F.resize(img, [new_h, new_w], interpolation=InterpolationMode.BILINEAR)\n",
    "        sx, sy = new_w / w, new_h / h\n",
    "\n",
    "        boxes = target['boxes'].clone()\n",
    "        boxes[:, [0,2]] *= sx\n",
    "        boxes[:, [1,3]] *= sy\n",
    "        boxes[:, [0,2]].clamp_(0, new_w)\n",
    "        boxes[:, [1,3]].clamp_(0, new_h)\n",
    "        keep = (boxes[:,2] - boxes[:,0] > 0) & (boxes[:,3] - boxes[:,1] > 0)\n",
    "        boxes = boxes[keep]\n",
    "\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = target['labels'][keep]\n",
    "\n",
    "        img = ToTensor()(img)\n",
    "        img = Normalize(mean, std)(img)\n",
    "        return img, target\n",
    "\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:05:42.460572Z",
     "iopub.status.busy": "2025-05-01T01:05:42.460390Z",
     "iopub.status.idle": "2025-05-01T01:05:42.477743Z",
     "shell.execute_reply": "2025-05-01T01:05:42.477130Z",
     "shell.execute_reply.started": "2025-05-01T01:05:42.460558Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, root, image_set, transforms=None):\n",
    "        self.root = root\n",
    "        ids_file = os.path.join(root, \"ImageSets\", \"Main\", f\"{image_set}.txt\")\n",
    "        with open(ids_file) as f:\n",
    "            img_ids = [l.strip() for l in f if l.strip()]\n",
    "\n",
    "        self.annotations = []\n",
    "        skipped_images = 0\n",
    "        for img_id in img_ids:\n",
    "            xml_path = os.path.join(root, \"Annotations\", f\"{img_id}.xml\")\n",
    "            tree = ET.parse(xml_path)\n",
    "            boxes, labels = [], []\n",
    "            for obj in tree.getroot().findall(\"object\"):\n",
    "                cls = obj.find(\"name\").text\n",
    "                idx = CLASS_NAME_TO_IDX.get(cls)\n",
    "                if idx is None:\n",
    "                    continue\n",
    "                b = obj.find(\"bndbox\")\n",
    "                coords = [\n",
    "                    float(b.find(\"xmin\").text),\n",
    "                    float(b.find(\"ymin\").text),\n",
    "                    float(b.find(\"xmax\").text),\n",
    "                    float(b.find(\"ymax\").text),\n",
    "                ]\n",
    "                if coords[2] <= coords[0] or coords[3] <= coords[1]:\n",
    "                    continue\n",
    "                boxes.append(coords)\n",
    "                labels.append(idx)\n",
    "\n",
    "            if not labels:\n",
    "                print(f\"No labels found for image id: {img_id}\")\n",
    "                skipped_images = skipped_images+1\n",
    "                continue\n",
    "\n",
    "            self.annotations.append({\n",
    "                \"id\": img_id,\n",
    "                \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "                \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            })\n",
    "            \n",
    "        print(f\"{image_set}.txt: Skipped {skipped_images} images\")\n",
    "        assert self.annotations, f\"No annotations for split {image_set}\"\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ann = self.annotations[idx]\n",
    "        img_path = os.path.join(self.root, \"JPEGImages\", f\"{ann['id']}.jpg\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        target = {\n",
    "            \"boxes\": ann[\"boxes\"].clone(),\n",
    "            \"labels\": ann[\"labels\"].clone()\n",
    "        }\n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:05:42.478706Z",
     "iopub.status.busy": "2025-05-01T01:05:42.478479Z",
     "iopub.status.idle": "2025-05-01T01:06:41.756128Z",
     "shell.execute_reply": "2025-05-01T01:06:41.755479Z",
     "shell.execute_reply.started": "2025-05-01T01:05:42.478691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ds = VOCDataset(\n",
    "    args.voc_root,\n",
    "    args.train_set,\n",
    "    transforms=get_train_transform(\n",
    "        args.min_short_size, args.max_short_size,\n",
    "        args.max_long_size, mean, std\n",
    "    )\n",
    ")\n",
    "val_ds = VOCDataset(\n",
    "    args.voc_root,\n",
    "    args.val_set,\n",
    "    transforms=get_val_transform(\n",
    "        args.val_short_size, args.val_long_size, mean, std\n",
    "    )\n",
    ")\n",
    "print(f\"Train examples: {len(train_ds)}, Val examples: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:06:41.757302Z",
     "iopub.status.busy": "2025-05-01T01:06:41.756967Z",
     "iopub.status.idle": "2025-05-01T01:06:41.788110Z",
     "shell.execute_reply": "2025-05-01T01:06:41.787514Z",
     "shell.execute_reply.started": "2025-05-01T01:06:41.757278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "counts = dict.fromkeys(range(1, NUM_CLASSES), 0)\n",
    "for ann in train_ds.annotations:\n",
    "    for lbl in ann[\"labels\"].tolist():\n",
    "        counts[lbl] += 1\n",
    "for k, v in counts.items():\n",
    "    counts[k] = max(1, v)\n",
    "weights = [\n",
    "    sum(1.0 / counts[lbl] for lbl in ann[\"labels\"].tolist()) / len(ann[\"labels\"])\n",
    "    for ann in train_ds.annotations\n",
    "]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:06:41.789544Z",
     "iopub.status.busy": "2025-05-01T01:06:41.788813Z",
     "iopub.status.idle": "2025-05-01T01:06:41.796140Z",
     "shell.execute_reply": "2025-05-01T01:06:41.795338Z",
     "shell.execute_reply.started": "2025-05-01T01:06:41.789525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=sampler,\n",
    "    num_workers=args.workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.workers,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:06:41.797236Z",
     "iopub.status.busy": "2025-05-01T01:06:41.796955Z",
     "iopub.status.idle": "2025-05-01T01:06:48.189911Z",
     "shell.execute_reply": "2025-05-01T01:06:48.189155Z",
     "shell.execute_reply.started": "2025-05-01T01:06:41.797213Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = retinanet_resnet50_fpn(\n",
    "    weights=None,\n",
    "    weights_backbone=ResNet50_Weights.IMAGENET1K_V1,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    trainable_backbone_layers=args.trainable_backbone_layers\n",
    ").to(device)\n",
    "\n",
    "optimizer = SGD(\n",
    "    model.parameters(),\n",
    "    lr=args.lr,\n",
    "    momentum=args.momentum,\n",
    "    weight_decay=args.weight_decay\n",
    ")\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=args.lr,\n",
    "    epochs=args.epochs,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.3,\n",
    "    div_factor=25\n",
    ")\n",
    "scaler = torch.amp.GradScaler(device)\n",
    "\n",
    "print(f\"Starting LR: {scheduler.get_last_lr()[0]:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:06:48.190939Z",
     "iopub.status.busy": "2025-05-01T01:06:48.190725Z",
     "iopub.status.idle": "2025-05-01T01:06:48.205824Z",
     "shell.execute_reply": "2025-05-01T01:06:48.205022Z",
     "shell.execute_reply.started": "2025-05-01T01:06:48.190919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "if args.resume_checkpoint and os.path.exists(args.resume_checkpoint):\n",
    "    ck = torch.load(args.resume_checkpoint, map_location=device)\n",
    "    model.load_state_dict(ck[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(ck[\"optimizer_state_dict\"])\n",
    "    scheduler.load_state_dict(ck[\"scheduler_state_dict\"])\n",
    "    scaler.load_state_dict(ck.get(\"scaler_state_dict\", {}))\n",
    "    start_epoch = ck.get(\"epoch\", 1) + 1\n",
    "    print(f\"Resumed from checkpoint '{args.resume_checkpoint}', starting at epoch {start_epoch}\")\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scheduler, epoch, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_cls_loss = 0.0\n",
    "    total_box_loss = 0.0\n",
    "\n",
    "    for imgs, targets in tqdm(loader, desc=f\"Train Epoch {epoch}\", leave=False):\n",
    "        imgs = [img.to(device) for img in imgs]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device):\n",
    "            loss_dict = model(imgs, targets)\n",
    "            # split out the two losses\n",
    "            cls_loss = loss_dict['classification']\n",
    "            box_loss = loss_dict['bbox_regression']\n",
    "            loss = cls_loss + box_loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        # (optional) clip gradients here if you wish\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss     += loss.item()\n",
    "        total_cls_loss += cls_loss.item()\n",
    "        total_box_loss += box_loss.item()\n",
    "\n",
    "    n = len(loader)\n",
    "    avg_loss     = total_loss     / n\n",
    "    avg_cls_loss = total_cls_loss / n\n",
    "    avg_box_loss = total_box_loss / n\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    print(f\"Epoch {epoch} | Avg Train Loss: {avg_loss:.4f} | Avg Cls Loss: {avg_cls_loss:.4f} | Avg Box Loss: {avg_box_loss:.4f} | LR: {lr:.2e}\")\n",
    "\n",
    "    logger.report_scalar(\"Train/Loss/Total\",          \"epoch\", iteration=epoch, value=avg_loss)\n",
    "    logger.report_scalar(\"Train/Loss/Classification\", \"epoch\", iteration=epoch, value=avg_cls_loss)\n",
    "    logger.report_scalar(\"Train/Loss/BoxRegression\",  \"epoch\", iteration=epoch, value=avg_box_loss)\n",
    "    logger.report_scalar(\"Train/LearningRate\",        \"epoch\", iteration=epoch, value=lr)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, epoch):\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision(box_format='xyxy', class_metrics=True)\n",
    "\n",
    "    total_val_loss     = 0.0\n",
    "    total_val_cls_loss = 0.0\n",
    "    total_val_box_loss = 0.0\n",
    "\n",
    "    for imgs, targets in tqdm(loader, desc=f\"Val Epoch {epoch}\", leave=False):\n",
    "        imgs_cuda    = [img.to(device) for img in imgs]\n",
    "        targets_cuda = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        model.train()\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "\n",
    "        loss_dict = model(imgs_cuda, targets_cuda)\n",
    "        cls_loss = loss_dict['classification']\n",
    "        box_loss = loss_dict['bbox_regression']\n",
    "        batch_loss = (cls_loss + box_loss).item()\n",
    "\n",
    "        total_val_loss     += batch_loss\n",
    "        total_val_cls_loss += cls_loss.item()\n",
    "        total_val_box_loss += box_loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        outputs = model(imgs_cuda)\n",
    "        preds = [\n",
    "            {k: v.cpu() for k, v in out.items() if k in ('boxes','scores','labels')}\n",
    "            for out in outputs\n",
    "        ]\n",
    "        metric.update(preds, targets)\n",
    "\n",
    "    n = len(loader)\n",
    "    avg_val_loss     = total_val_loss     / n\n",
    "    avg_val_cls_loss = total_val_cls_loss / n\n",
    "    avg_val_box_loss = total_val_box_loss / n\n",
    "\n",
    "    results      = metric.compute()\n",
    "    overall_map  = results[\"map\"].item()\n",
    "    per_class_map = results[\"map_per_class\"]\n",
    "\n",
    "    print(f\"Epoch {epoch} | Avg Val Loss: {avg_val_loss:.4f} | Avg Cls Loss: {avg_val_cls_loss:.4f} | Avg Box Loss: {avg_val_box_loss:.4f} | mAP: {overall_map:.4f}\")\n",
    "\n",
    "    logger.report_scalar(\"Val/Loss/Total\",          \"epoch\", iteration=epoch, value=avg_val_loss)\n",
    "    logger.report_scalar(\"Val/Loss/Classification\", \"epoch\", iteration=epoch, value=avg_val_cls_loss)\n",
    "    logger.report_scalar(\"Val/Loss/BoxRegression\",  \"epoch\", iteration=epoch, value=avg_val_box_loss)\n",
    "    logger.report_scalar(\"Val/Detection/mAP\",       \"epoch\", iteration=epoch, value=overall_map)\n",
    "\n",
    "    for idx, ap in enumerate(per_class_map):\n",
    "        cls = CLASS_NAMES[idx]\n",
    "        logger.report_scalar(\"Val/Detection/AP\", cls, iteration=epoch, value=ap.item())\n",
    "\n",
    "    return avg_val_loss, overall_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:06:48.207162Z",
     "iopub.status.busy": "2025-05-01T01:06:48.206706Z",
     "iopub.status.idle": "2025-05-01T01:06:48.355095Z",
     "shell.execute_reply": "2025-05-01T01:06:48.354279Z",
     "shell.execute_reply.started": "2025-05-01T01:06:48.207137Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def log_head_histograms(epoch, model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if (\"head.classification_head\" in name) or (\"head.regression_head\" in name):\n",
    "            if param.grad is not None:\n",
    "                logger.report_histogram(\n",
    "                    title=\"Gradients\",\n",
    "                    series=name,\n",
    "                    iteration=epoch,\n",
    "                    values=param.grad.detach().cpu().numpy().ravel(),\n",
    "                    data_args={\"nbinsx\": 50},        # Plotlyâ€™s nbinsx parameter\n",
    "                )\n",
    "            logger.report_histogram(\n",
    "                title=\"Weights\",\n",
    "                series=name,\n",
    "                iteration=epoch,\n",
    "                values=param.data.detach().cpu().numpy().ravel(),\n",
    "                data_args={\"nbinsx\": 50},\n",
    "            )\n",
    "\n",
    "\n",
    "def log_sanity_images(epoch, model, val_loader, args, mean, std):\n",
    "    model.eval()\n",
    "    imgs, targets = next(iter(val_loader))\n",
    "    imgs_cuda = [img.to(device) for img in imgs]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(imgs_cuda)\n",
    "\n",
    "    save_dir = os.path.join(args.output_dir, \"sanity_images\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for i, (img_tensor, tgt, pred) in enumerate(zip(imgs, targets, outputs)):\n",
    "        img = img_tensor.clone()\n",
    "        for c, m, s in zip(img, mean, std):\n",
    "            c.mul_(s).add_(m)\n",
    "        arr = (img.permute(1,2,0).cpu().numpy() * 255).astype(\"uint8\")\n",
    "        pil_img = Image.fromarray(arr)\n",
    "\n",
    "        draw = ImageDraw.Draw(pil_img)\n",
    "        for box in tgt[\"boxes\"]:\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=\"green\", width=2)\n",
    "        for box in pred[\"boxes\"].cpu():\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "\n",
    "        path = os.path.join(save_dir, f\"epoch{epoch}_sample{i}.png\")\n",
    "        pil_img.save(path)\n",
    "        logger.report_image(\n",
    "            title=\"Val Sample Detections\",\n",
    "            series=f\"epoch_{epoch}\",\n",
    "            local_path=path,\n",
    "            iteration=epoch\n",
    "        )\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T01:06:48.356126Z",
     "iopub.status.busy": "2025-05-01T01:06:48.355897Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_map = 0.0\n",
    "for epoch in range(start_epoch, args.epochs + 1):\n",
    "    epoch_start = time.time()\n",
    "    train_loss = train_one_epoch(\n",
    "        model, train_loader, optimizer, scheduler, epoch, scaler, device\n",
    "    )\n",
    "    \n",
    "    val_loss, val_map = evaluate(model, val_loader, epoch)\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        log_head_histograms(epoch, model)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        log_sanity_images(epoch, model, val_loader, args, mean, std)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    logger.report_scalar(\n",
    "        title=\"Time/Epoch\",\n",
    "        series=\"duration_seconds\",\n",
    "        iteration=epoch,\n",
    "        value=epoch_time\n",
    "    )\n",
    "    if val_map > best_map:\n",
    "        best_map = val_map\n",
    "        best_ckpt = os.path.join(args.output_dir, \"best_model.pth\")\n",
    "        torch.save({\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"scaler_state_dict\": scaler.state_dict(),\n",
    "            \"epoch\": epoch\n",
    "        }, best_ckpt)\n",
    "        print(f\"Epoch {epoch}: new best mAP = {best_map:.4f}, saved to {best_ckpt}\")\n",
    "        task.upload_artifact(name=\"best_model.pth\", artifact_object=best_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "final_path = os.path.join(args.output_dir, \"final_model.pth\")\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "    \"scaler_state_dict\": scaler.state_dict(),\n",
    "    \"epoch\": args.epochs\n",
    "}, final_path)\n",
    "print(f\"Final model weights saved to {final_path}\")\n",
    "\n",
    "best_map_path = os.path.join(args.output_dir, \"best_map.txt\")\n",
    "with open(best_map_path, \"w\") as f:\n",
    "    f.write(f\"best_map: {best_map:.4f}\\n\")\n",
    "\n",
    "task.upload_artifact(name=\"final_model.pth\", artifact_object=final_path)\n",
    "task.upload_artifact(name=\"best_map.txt\", artifact_object=best_map_path)\n",
    "\n",
    "task.close()\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 35388,
     "sourceId": 47853,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
